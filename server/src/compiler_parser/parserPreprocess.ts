import {TokenKind, TokenObject, TokenString} from "../compiler_tokenizer/tokenObject";
import {diagnostic} from "../core/diagnostic";
import {HighlightForToken} from "../core/highlight";
import {TokenRange} from "../compiler_tokenizer/tokenRange";

/**
 * Output of the 'preprocessAfterTokenized' function.
 */
export interface PreprocessedOutput {
    readonly preprocessedTokens: TokenObject[];
    readonly includePathTokens: TokenString[];
}

/**
 * Preprocess the token list for the parser.
 * For example, remove comments, concatenate continuous strings, and process directives.
 * @param tokens Tokens generated by the tokenizer.
 */
export function preprocessAfterTokenized(tokens: TokenObject[]): PreprocessedOutput {
    // Remove comments
    const actualTokens: TokenObject[] = tokens.filter(t => t.kind !== TokenKind.Comment);

    // Handle preprocessor directives
    const includeFiles = preprocessDirectives(actualTokens);

    // Concatenate continuous strings.
    for (let i = actualTokens.length - 1; i >= 1; i--) {
        const canCombine =
            actualTokens[i - 1].isStringToken() && actualTokens[i].isStringToken(); // "string" --> "string"
        if (canCombine === false) continue;

        // Create a new token with the combined elements.
        actualTokens[i - 1] =
            createCombinedStringToken(actualTokens[i - 1] as TokenString, actualTokens[i] as TokenString); // "stringstring"
        actualTokens.splice(i, 1);
    }

    // Assign index information.
    for (let i = 0; i < actualTokens.length; i++) {
        actualTokens[i].bindPreprocessedToken(i, i != actualTokens.length - 1 ? actualTokens[i + 1] : undefined);
    }

    return {
        preprocessedTokens: actualTokens,
        includePathTokens: includeFiles
    };
}

function preprocessDirectives(tokens: TokenObject[]): TokenString[] {
    const includeFiles: TokenString[] = [];
    const directiveRanges: [number, number][] = [];

    // Handle preprocessor directives starting with '#'
    for (let i = 0; i < tokens.length; i++) {
        if (tokens[i].text !== '#') continue;
        const directiveTokens = sliceTokenListBySameLine(tokens, i);

        handleDirectiveTokens(directiveTokens, includeFiles);
        directiveRanges.push([i, directiveTokens.length]);
    }

    // Remove preprocessor directives.
    for (let i = directiveRanges.length - 1; i >= 0; i--) {
        tokens.splice(directiveRanges[i][0], directiveRanges[i][1]);
    }

    return includeFiles;
}

function handleDirectiveTokens(directiveTokens: TokenObject[], includeFiles: TokenString[]) {
    directiveTokens[0].setHighlight(HighlightForToken.Directive);

    if (directiveTokens[1]?.text === 'include') {
        directiveTokens[1].setHighlight(HighlightForToken.Directive);

        // Check the include directive.
        const fileName = directiveTokens[2];
        if (fileName === undefined) {
            diagnostic.error(directiveTokens[1].location, 'Expected file name for include directive.');
            return;
        }

        if (fileName.isStringToken() === false) {
            diagnostic.error(directiveTokens[2].location, 'Expected string literal for include directive.');
            return;
        }

        includeFiles.push(fileName);
    } else {
        if (directiveTokens[1] != null) directiveTokens[1].setHighlight(HighlightForToken.Label);
    }
}

function sliceTokenListBySameLine(tokens: TokenObject[], head: number): TokenObject[] {
    let tail = head;
    for (let i = head; i < tokens.length - 1; i++) {
        if (tokens[i].location.end.isSameLine(tokens[i + 1].location.start) === false) {
            break;
        }

        tail = i + 1;
    }

    return tokens.slice(head, tail + 1);
}

function createCombinedStringToken(lhs: TokenString, rhs: TokenString): TokenObject {
    const tokenText = '"' + lhs.getStringContent() + rhs.getStringContent() + '"'; // FIXME?
    return TokenString.createVirtual(tokenText, new TokenRange(lhs, rhs));
}
